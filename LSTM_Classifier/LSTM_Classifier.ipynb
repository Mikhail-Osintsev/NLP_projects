{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "import pymorphy2\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.models import KeyedVectors\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import normalize\n",
    "from bokeh.io import output_notebook\n",
    "import bokeh.models as bm\n",
    "import bokeh.plotting as pl\n",
    "import umap.umap_ as umap\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP_WORDS = set(stopwords.words('english'))\n",
    "morph = pymorphy2.MorphAnalyzer(lang='uk')\n",
    "model_path = \"word2vec-google-news-300.model\"\n",
    "BATCH_SIZE = 64\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.1)\n",
    "NUM_EPOCHS = 5\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.load_dataset(\"ag_news\")\n",
    "wor2vec_model = KeyedVectors.load(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создаем класс, который делает предобработку текста, разбивку на токены, превращение предложений в эмбеддинги"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset():\n",
    "    def __init__(self, dataset,word2vec_model, fine_tune=False ):\n",
    "        self.dataset = dataset\n",
    "        self.fine_tune = fine_tune\n",
    "        self.corpus = [self.clean(self.dataset[idx]['text']).split() for idx in range(len(self.dataset))]\n",
    "        self.word2vec_model = word2vec_model\n",
    "\n",
    "        if self.fine_tune:\n",
    "            # Инициализация модели с параметрами исходной модели\n",
    "            self.word2vec_model = Word2Vec(vector_size=wor2vec_model.vector_size, min_count=1)\n",
    "            \n",
    "            # Инициализация словаря новыми словами\n",
    "            self.word2vec_model.build_vocab(self.corpus, update=True)\n",
    "            \n",
    "            # Копируем предобученные вектора в текущую модель\n",
    "            self.word2vec_model.wv.vectors = np.vstack(\n",
    "                [wor2vec_model[word] if word in wor2vec_model else np.random.uniform(-1, 1, wor2vec_model.vector_size)\n",
    "                 for word in self.word2vec_model.wv.index_to_key]\n",
    "            )\n",
    "            \n",
    "            # Продолжаем дообучение модели на новом корпусе\n",
    "            self.word2vec_model.train(self.corpus, total_examples=len(self.corpus), epochs=10)\n",
    "        \n",
    "\n",
    "    def clean(self, text):\n",
    "        # Очистка текста от пунктуации и нормализация\n",
    "        text = text.lower()\n",
    "        for i in punctuation:\n",
    "            text = text.replace(i, '')\n",
    "        text = [word for word in text.split() if word not in STOP_WORDS and not word.isdigit()]\n",
    "        text = [morph.parse(word)[0].normal_form for word in text]\n",
    "        return ' '.join(text)\n",
    "\n",
    "    def embedded_text(self, text):\n",
    "        # Получение эмбеддингов для текста\n",
    "        embed_tensor = torch.tensor([\n",
    "            self.word2vec_model[word] for word in text.split() if word in self.word2vec_model\n",
    "        ])\n",
    "        return embed_tensor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        cleaned_item = self.dataset[idx]['text']\n",
    "        cleaned_item = self.clean(cleaned_item)\n",
    "        cleaned_item = self.embedded_text(cleaned_item)    \n",
    "        return cleaned_item, self.dataset[idx]['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(dataset['train'], wor2vec_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рисуем график эмбеддингов\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Для отображения графика внутри Jupyter Notebook\n",
    "output_notebook()\n",
    "\n",
    "# Извлечение уникальных слов из train_dataset\n",
    "all_words = set()\n",
    "for idx in range(len(train_dataset)):\n",
    "    cleaned_text = train_dataset.clean(train_dataset.dataset[idx]['text'])\n",
    "    words = cleaned_text.split()\n",
    "    all_words.update(words)\n",
    "\n",
    "# Получение эмбеддингов для уникальных слов\n",
    "word_embeddings = []\n",
    "filtered_words = []  # слова, которые есть в модели\n",
    "for word in all_words:\n",
    "    if word in train_dataset.word2vec_model:\n",
    "        word_embeddings.append(train_dataset.word2vec_model[word])\n",
    "        filtered_words.append(word)\n",
    "\n",
    "# Применение UMAP для снижения размерности\n",
    "embedding_2d = umap.UMAP(n_neighbors=10, min_dist=0.1, metric='cosine').fit_transform(word_embeddings)\n",
    "\n",
    "# Функция для рисования графика с Bokeh\n",
    "def draw_vectors(\n",
    "    x,\n",
    "    y,\n",
    "    radius=10,\n",
    "    alpha=0.5,\n",
    "    color=\"blue\",\n",
    "    width=800,\n",
    "    height=600,\n",
    "    show=True,\n",
    "    **kwargs,\n",
    "):\n",
    "    \"\"\"Рисует интерактивный график с данными и отображением дополнительной информации при наведении.\"\"\"\n",
    "    if isinstance(color, str):\n",
    "        color = [color] * len(x)\n",
    "    data_source = bm.ColumnDataSource({\"x\": x, \"y\": y, \"color\": color, **kwargs})\n",
    "\n",
    "    fig = pl.figure(active_scroll=\"wheel_zoom\", width=width, height=height)\n",
    "    fig.scatter(\"x\", \"y\", size=radius, color=\"color\", alpha=alpha, source=data_source)\n",
    "\n",
    "    fig.add_tools(bm.HoverTool(tooltips=[(key, \"@\" + key) for key in kwargs.keys()]))\n",
    "    if show:\n",
    "        pl.show(fig)\n",
    "    return fig\n",
    "\n",
    "# Визуализация\n",
    "draw_vectors(\n",
    "    embedding_2d[:, 0], \n",
    "    embedding_2d[:, 1], \n",
    "    radius=10,\n",
    "    alpha=0.6,\n",
    "    color=\"blue\",\n",
    "    token=filtered_words  # добавляем слова для отображения в tooltip\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    text, labels = zip(*batch)\n",
    "    padded_texts = pad_sequence(text, batch_first=True, padding_value=0)\n",
    "    labels = torch.tensor(labels)\n",
    "    lengths = torch.tensor([len(seq) for seq in text])\n",
    "    return padded_texts, labels, lengths\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,collate_fn=collate_fn)\n",
    "\n",
    "for text, label, length in dataloader:\n",
    "    print(text)\n",
    "    print(label)\n",
    "    print(length)\n",
    "    print(text.shape)\n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BidirLSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BidirLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size=300,\n",
    "                            hidden_size=100,\n",
    "                            num_layers=2,\n",
    "                            bias=True,\n",
    "                            batch_first=True,\n",
    "                            \n",
    "                            bidirectional=True)\n",
    "        \n",
    "        self.linear = nn.Linear(in_features=200, out_features=4)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        packed_input = pack_padded_sequence(x, lengths, batch_first=True, enforce_sorted=False)\n",
    "        _, (h_n, _) = self.lstm(packed_input)\n",
    "        out = torch.cat((h_n[-2], h_n[-1]), dim=1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "model = BidirLSTM()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train(model, criterion, dataloader, optimizer, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        print(f'Start Epoch {epoch + 1}/{epochs}')\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        \n",
    "        for texts, labels, lengths in tqdm(dataloader, desc=f'Epoch {epoch + 1}/{epochs}', leave=False):\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(texts, lengths)\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        print(f'EPOCH {epoch + 1} / {epochs}, Loss: {avg_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model, criterion, dataloader, optimizer, NUM_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_dataset = CustomDataset(dataset['test'], wor2vec_model)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "\n",
    "def evaluate(model, dataloader):\n",
    "    model.eval() \n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for texts, labels, lengths in tqdm(dataloader):\n",
    "            texts, labels = texts.to(device), labels.to(device)\n",
    "            outputs = model(texts, lengths)\n",
    "            _, predicted = torch.max(outputs, dim=1)\n",
    "            \n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "    \n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    print(f'Test Accuracy: {accuracy:.4f}')\n",
    "\n",
    "\n",
    "evaluate(model, test_dataloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (py310)",
   "language": "python",
   "name": "py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
