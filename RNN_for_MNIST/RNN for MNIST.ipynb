{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import datasets, transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),                \n",
    "    transforms.Normalize((0.5,), (0.5,))  \n",
    "])\n",
    "\n",
    "# Загрузка данных MNIST\n",
    "training_data = datasets.MNIST(root='data', train=True, download=True, transform=transform)\n",
    "test_data = datasets.MNIST(root='data', train=False, download=True, transform=transform)\n",
    "train_dataloader = DataLoader(training_data, batch_size=64)\n",
    "train_dataloader = DataLoader(test_data, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        \n",
    "        self.rnn_layer = nn.RNN(input_size=28, hidden_size=30, num_layers=2, batch_first=True)\n",
    "        self.output_layer =nn.Linear(30, 10)\n",
    "\n",
    "    \n",
    "    def forward(self, X):\n",
    "        X = X.squeeze(1) \n",
    "        X, _ = self.rnn_layer(X)\n",
    "        X = self.output_layer(X[:, -1, :])\n",
    "        return X\n",
    "    \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNet()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "batch_size = 64\n",
    "epochs = 10\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    total_loss = 0.0  \n",
    "    \n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        \n",
    "        pred = model(X)\n",
    "        \n",
    "        \n",
    "        loss = loss_fn(pred, y)\n",
    "        total_loss += loss.detach().item()  \n",
    "\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        \n",
    "        if batch % 100 == 0:\n",
    "            current = batch * len(X)\n",
    "            print(f'Current loss is {loss.item():.4f}, {current} / {size}')\n",
    "\n",
    "    \n",
    "    average_loss = total_loss / len(dataloader)\n",
    "    return average_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 / 11\n",
      "Current loss is 2.2727, 0 / 10000\n",
      "Current loss is 2.3165, 6400 / 10000\n",
      "Epoch 1 finished with average training loss: 2.3171\n",
      "------------------------------\n",
      "Epoch 1 / 11\n",
      "Current loss is 2.2727, 0 / 10000\n",
      "Current loss is 2.3165, 6400 / 10000\n",
      "Epoch 2 finished with average training loss: 2.3171\n",
      "------------------------------\n",
      "Epoch 2 / 11\n",
      "Current loss is 2.2727, 0 / 10000\n",
      "Current loss is 2.3165, 6400 / 10000\n",
      "Epoch 3 finished with average training loss: 2.3171\n",
      "------------------------------\n",
      "Epoch 3 / 11\n",
      "Current loss is 2.2727, 0 / 10000\n",
      "Current loss is 2.3165, 6400 / 10000\n",
      "Epoch 4 finished with average training loss: 2.3171\n",
      "------------------------------\n",
      "Epoch 4 / 11\n",
      "Current loss is 2.2727, 0 / 10000\n",
      "Current loss is 2.3165, 6400 / 10000\n",
      "Epoch 5 finished with average training loss: 2.3171\n",
      "------------------------------\n",
      "Epoch 5 / 11\n",
      "Current loss is 2.2727, 0 / 10000\n",
      "Current loss is 2.3165, 6400 / 10000\n",
      "Epoch 6 finished with average training loss: 2.3171\n",
      "------------------------------\n",
      "Epoch 6 / 11\n",
      "Current loss is 2.2727, 0 / 10000\n",
      "Current loss is 2.3165, 6400 / 10000\n",
      "Epoch 7 finished with average training loss: 2.3171\n",
      "------------------------------\n",
      "Epoch 7 / 11\n",
      "Current loss is 2.2727, 0 / 10000\n",
      "Current loss is 2.3165, 6400 / 10000\n",
      "Epoch 8 finished with average training loss: 2.3171\n",
      "------------------------------\n",
      "Epoch 8 / 11\n",
      "Current loss is 2.2727, 0 / 10000\n",
      "Current loss is 2.3165, 6400 / 10000\n",
      "Epoch 9 finished with average training loss: 2.3171\n",
      "------------------------------\n",
      "Epoch 9 / 11\n",
      "Current loss is 2.2727, 0 / 10000\n",
      "Current loss is 2.3165, 6400 / 10000\n",
      "Epoch 10 finished with average training loss: 2.3171\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    print(f'Epoch {epoch} / {epochs + 1}')\n",
    "    train_loss = train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    print(f\"Epoch {epoch + 1} finished with average training loss: {train_loss:.4f}\")\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 / 10\n",
      "Current loss is 2.3186, 0 / 60000\n",
      "Current loss is 1.0752, 6400 / 60000\n",
      "Current loss is 0.9785, 12800 / 60000\n",
      "Current loss is 0.7817, 19200 / 60000\n",
      "Current loss is 0.6653, 25600 / 60000\n",
      "Current loss is 0.5857, 32000 / 60000\n",
      "Current loss is 0.3266, 38400 / 60000\n",
      "Current loss is 0.5631, 44800 / 60000\n",
      "Current loss is 0.3852, 51200 / 60000\n",
      "Current loss is 0.2867, 57600 / 60000\n",
      "Epoch 1 finished with average training loss: 0.7203\n",
      "------------------------------\n",
      "Epoch 2 / 10\n",
      "Current loss is 0.2583, 0 / 60000\n",
      "Current loss is 0.2979, 6400 / 60000\n",
      "Current loss is 0.3834, 12800 / 60000\n",
      "Current loss is 0.2032, 19200 / 60000\n",
      "Current loss is 0.3610, 25600 / 60000\n",
      "Current loss is 0.2329, 32000 / 60000\n",
      "Current loss is 0.4524, 38400 / 60000\n",
      "Current loss is 0.4718, 44800 / 60000\n",
      "Current loss is 0.2208, 51200 / 60000\n",
      "Current loss is 0.1346, 57600 / 60000\n",
      "Epoch 2 finished with average training loss: 0.3052\n",
      "------------------------------\n",
      "Epoch 3 / 10\n",
      "Current loss is 0.2930, 0 / 60000\n",
      "Current loss is 0.3012, 6400 / 60000\n",
      "Current loss is 0.1938, 12800 / 60000\n",
      "Current loss is 0.2245, 19200 / 60000\n",
      "Current loss is 0.2815, 25600 / 60000\n",
      "Current loss is 0.2031, 32000 / 60000\n",
      "Current loss is 0.1541, 38400 / 60000\n",
      "Current loss is 0.1697, 44800 / 60000\n",
      "Current loss is 0.1950, 51200 / 60000\n",
      "Current loss is 0.2059, 57600 / 60000\n",
      "Epoch 3 finished with average training loss: 0.2336\n",
      "------------------------------\n",
      "Epoch 4 / 10\n",
      "Current loss is 0.3411, 0 / 60000\n",
      "Current loss is 0.1856, 6400 / 60000\n",
      "Current loss is 0.0941, 12800 / 60000\n",
      "Current loss is 0.1214, 19200 / 60000\n",
      "Current loss is 0.1361, 25600 / 60000\n",
      "Current loss is 0.5034, 32000 / 60000\n",
      "Current loss is 0.3769, 38400 / 60000\n",
      "Current loss is 0.0953, 44800 / 60000\n",
      "Current loss is 0.0478, 51200 / 60000\n",
      "Current loss is 0.3046, 57600 / 60000\n",
      "Epoch 4 finished with average training loss: 0.2004\n",
      "------------------------------\n",
      "Epoch 5 / 10\n",
      "Current loss is 0.1324, 0 / 60000\n",
      "Current loss is 0.2640, 6400 / 60000\n",
      "Current loss is 0.1440, 12800 / 60000\n",
      "Current loss is 0.0923, 19200 / 60000\n",
      "Current loss is 0.1628, 25600 / 60000\n",
      "Current loss is 0.4778, 32000 / 60000\n",
      "Current loss is 0.2602, 38400 / 60000\n",
      "Current loss is 0.1039, 44800 / 60000\n",
      "Current loss is 0.1252, 51200 / 60000\n",
      "Current loss is 0.0362, 57600 / 60000\n",
      "Epoch 5 finished with average training loss: 0.1756\n",
      "------------------------------\n",
      "Epoch 6 / 10\n",
      "Current loss is 0.0881, 0 / 60000\n",
      "Current loss is 0.0983, 6400 / 60000\n",
      "Current loss is 0.0958, 12800 / 60000\n",
      "Current loss is 0.1328, 19200 / 60000\n",
      "Current loss is 0.2429, 25600 / 60000\n",
      "Current loss is 0.1296, 32000 / 60000\n",
      "Current loss is 0.2477, 38400 / 60000\n",
      "Current loss is 0.1838, 44800 / 60000\n",
      "Current loss is 0.0437, 51200 / 60000\n",
      "Current loss is 0.2313, 57600 / 60000\n",
      "Epoch 6 finished with average training loss: 0.1593\n",
      "------------------------------\n",
      "Epoch 7 / 10\n",
      "Current loss is 0.1217, 0 / 60000\n",
      "Current loss is 0.0681, 6400 / 60000\n",
      "Current loss is 0.2241, 12800 / 60000\n",
      "Current loss is 0.0313, 19200 / 60000\n",
      "Current loss is 0.1274, 25600 / 60000\n",
      "Current loss is 0.1497, 32000 / 60000\n",
      "Current loss is 0.0699, 38400 / 60000\n",
      "Current loss is 0.0683, 44800 / 60000\n",
      "Current loss is 0.0757, 51200 / 60000\n",
      "Current loss is 0.1366, 57600 / 60000\n",
      "Epoch 7 finished with average training loss: 0.1498\n",
      "------------------------------\n",
      "Epoch 8 / 10\n",
      "Current loss is 0.1017, 0 / 60000\n",
      "Current loss is 0.1594, 6400 / 60000\n",
      "Current loss is 0.1895, 12800 / 60000\n",
      "Current loss is 0.2077, 19200 / 60000\n",
      "Current loss is 0.1932, 25600 / 60000\n",
      "Current loss is 0.0281, 32000 / 60000\n",
      "Current loss is 0.0863, 38400 / 60000\n",
      "Current loss is 0.1993, 44800 / 60000\n",
      "Current loss is 0.1645, 51200 / 60000\n",
      "Current loss is 0.2164, 57600 / 60000\n",
      "Epoch 8 finished with average training loss: 0.1348\n",
      "------------------------------\n",
      "Epoch 9 / 10\n",
      "Current loss is 0.0454, 0 / 60000\n",
      "Current loss is 0.3086, 6400 / 60000\n",
      "Current loss is 0.0900, 12800 / 60000\n",
      "Current loss is 0.0200, 19200 / 60000\n",
      "Current loss is 0.0230, 25600 / 60000\n",
      "Current loss is 0.0923, 32000 / 60000\n",
      "Current loss is 0.2441, 38400 / 60000\n",
      "Current loss is 0.0659, 44800 / 60000\n",
      "Current loss is 0.1266, 51200 / 60000\n",
      "Current loss is 0.1145, 57600 / 60000\n",
      "Epoch 9 finished with average training loss: 0.1333\n",
      "------------------------------\n",
      "Epoch 10 / 10\n",
      "Current loss is 0.1239, 0 / 60000\n",
      "Current loss is 0.0066, 6400 / 60000\n",
      "Current loss is 0.2183, 12800 / 60000\n",
      "Current loss is 0.0203, 19200 / 60000\n",
      "Current loss is 0.0802, 25600 / 60000\n",
      "Current loss is 0.0817, 32000 / 60000\n",
      "Current loss is 0.1188, 38400 / 60000\n",
      "Current loss is 0.1260, 44800 / 60000\n",
      "Current loss is 0.1492, 51200 / 60000\n",
      "Current loss is 0.0496, 57600 / 60000\n",
      "Epoch 10 finished with average training loss: 0.1240\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Преобразования для данных\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),                # Преобразование в тензор\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Нормализация данных в диапазон [-1, 1]\n",
    "])\n",
    "\n",
    "# Загрузка данных MNIST\n",
    "training_data = datasets.MNIST(root='data', train=True, download=True, transform=transform)\n",
    "test_data = datasets.MNIST(root='data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Определение модели\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.rnn_layer = nn.RNN(input_size=28, hidden_size=64, num_layers=2, batch_first=True)\n",
    "        self.output_layer = nn.Linear(64, 10)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = X.squeeze(1)  # Убираем лишнюю размерность\n",
    "        X, _ = self.rnn_layer(X)\n",
    "        X = self.output_layer(X[:, -1, :])\n",
    "        return X\n",
    "\n",
    "# Инициализация модели, даталоадеров, функции потерь и оптимизатора\n",
    "model = NeuralNet()\n",
    "train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64)\n",
    "\n",
    "learning_rate = 1e-3\n",
    "epochs = 10\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Функция для обучения\n",
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Предсказание и вычисление потерь\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        total_loss += loss.detach().item()\n",
    "\n",
    "        # Шаг градиентного спуска\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Печать каждые 100 шагов\n",
    "        if batch % 100 == 0:\n",
    "            current = batch * len(X)\n",
    "            print(f\"Current loss is {loss.item():.4f}, {current} / {size}\")\n",
    "\n",
    "    average_loss = total_loss / len(dataloader)\n",
    "    return average_loss\n",
    "\n",
    "# Цикл по эпохам\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch + 1} / {epochs}\")\n",
    "    train_loss = train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    print(f\"Epoch {epoch + 1} finished with average training loss: {train_loss:.4f}\")\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()  # Переключаем модель в режим оценки\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():  # Отключаем автоматическое вычисление градиентов\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()  # Суммируем потери для каждого батча\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()  # Считаем количество правильных предсказаний\n",
    "\n",
    "    test_loss /= num_batches  \n",
    "    accuracy = correct / size  \n",
    "    print(f\"Test Error: \\n Accuracy: {(100 * accuracy):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 96.5%, Avg loss: 0.118615 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_loss = test_loop(test_dataloader, model, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted label: 7\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "# Функция для отображения изображения\n",
    "def show_image(image, title=\"Image\"):\n",
    "    image = image.squeeze(0)  # Убираем лишний размер, если он есть (например, из-за batch dimension)\n",
    "    plt.imshow(image, cmap=\"gray\")\n",
    "    plt.title(title)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "# Получаем одну картинку из тестового датасета\n",
    "example_image, example_label = test_data[0]\n",
    "\n",
    "# Показать изображение с правильной меткой\n",
    "show_image(example_image, title=f\"True label: {example_label}\")\n",
    "\n",
    "# Подготовка изображения для передачи в модель\n",
    "# Добавляем batch dimension (чтобы получить форму [1, 1, 28, 28], если это MNIST)\n",
    "input_image = example_image.unsqueeze(0)  # Изменяем размер с [1, 28, 28] на [1, 1, 28, 28]\n",
    "\n",
    "# Переключаем модель в режим оценки\n",
    "model.eval()\n",
    "\n",
    "# Провести изображение через модель\n",
    "with torch.no_grad():  # Выключаем градиенты для режима оценки\n",
    "    output = model(input_image)\n",
    "    predicted_class = output.argmax(dim=1).item()  # Получаем индекс класса с максимальной вероятностью\n",
    "\n",
    "# Показать результат предсказания\n",
    "print(f\"Predicted label: {predicted_class}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (py310)",
   "language": "python",
   "name": "py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
